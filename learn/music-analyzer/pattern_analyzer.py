import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class MusicPatternAnalyzer:
    def __init__(self, csv_file='analysis_results.csv'):
        """åˆå§‹åŒ–éŸ³ä¹æ¨¡å¼åˆ†æå™¨"""
        self.df = pd.read_csv(csv_file)
        self.feature_columns = [col for col in self.df.columns if col != 'song']
        print(f"å·²åŠ è½½ {len(self.df)} é¦–æ­Œæ›²çš„æ•°æ®")
        print(f"ç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\n=== åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ ===")
        
        # BPMåˆ†æ
        bpm_stats = self.df['bpm'].describe()
        print(f"\nBPMç»Ÿè®¡:")
        print(f"å¹³å‡BPM: {bpm_stats['mean']:.1f}")
        print(f"BPMèŒƒå›´: {bpm_stats['min']:.1f} - {bpm_stats['max']:.1f}")
        print(f"BPMæ ‡å‡†å·®: {bpm_stats['std']:.1f}")
        
        # èŠ‚å¥å¼ºåº¦åˆ†æ
        onset_stats = self.df['onset_mean'].describe()
        print(f"\nèŠ‚å¥å¼ºåº¦ç»Ÿè®¡:")
        print(f"å¹³å‡èŠ‚å¥å¼ºåº¦: {onset_stats['mean']:.3f}")
        print(f"èŠ‚å¥å¼ºåº¦èŒƒå›´: {onset_stats['min']:.3f} - {onset_stats['max']:.3f}")
        
        return bpm_stats, onset_stats
    
    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ - å‘ç°ç‰¹å¾ä¹‹é—´çš„å…³ç³»"""
        print("\n=== ç‰¹å¾ç›¸å…³æ€§åˆ†æ ===")
        
        # è®¡ç®—ç›¸å…³çŸ©é˜µ
        correlation_matrix = self.df[self.feature_columns].corr()
        
        # æ‰¾å‡ºå¼ºç›¸å…³çš„ç‰¹å¾å¯¹
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # å¼ºç›¸å…³é˜ˆå€¼
                    strong_correlations.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        print("å¼ºç›¸å…³ç‰¹å¾å¯¹:")
        for corr in sorted(strong_correlations, key=lambda x: abs(x['correlation']), reverse=True):
            print(f"{corr['feature1']} <-> {corr['feature2']}: {corr['correlation']:.3f}")
        
        # å¯è§†åŒ–ç›¸å…³çŸ©é˜µ
        plt.figure(figsize=(15, 12))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
        plt.title('éŸ³ä¹ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
        plt.tight_layout()
        plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return correlation_matrix
    
    def clustering_analysis(self, n_clusters=3):
        """èšç±»åˆ†æ - å‘ç°ç›¸ä¼¼çš„éŸ³ä¹é£æ ¼"""
        print(f"\n=== èšç±»åˆ†æ (åˆ†ä¸º{n_clusters}ç±») ===")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(self.df[self.feature_columns])
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(features_scaled)
        
        # æ·»åŠ èšç±»ç»“æœåˆ°æ•°æ®æ¡†
        self.df['cluster'] = clusters
        
        # åˆ†ææ¯ä¸ªèšç±»çš„ç‰¹å¾
        for i in range(n_clusters):
            cluster_data = self.df[self.df['cluster'] == i]
            print(f"\nèšç±» {i} ({len(cluster_data)} é¦–æ­Œ):")
            print("æ­Œæ›²:", list(cluster_data['song']))
            print(f"å¹³å‡BPM: {cluster_data['bpm'].mean():.1f}")
            print(f"å¹³å‡èŠ‚å¥å¼ºåº¦: {cluster_data['onset_mean'].mean():.3f}")
            print(f"å¹³å‡æ¢…å°”é¢‘è°±: {cluster_data['mel_mean'].mean():.3f}")
        
        # å¯è§†åŒ–èšç±»ç»“æœ (ä½¿ç”¨PCAé™ç»´)
        pca = PCA(n_components=2)
        features_pca = pca.fit_transform(features_scaled)
        
        plt.figure(figsize=(10, 8))
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        for i in range(n_clusters):
            mask = clusters == i
            plt.scatter(features_pca[mask, 0], features_pca[mask, 1], 
                       c=colors[i], label=f'èšç±» {i}', alpha=0.7, s=100)
        
        plt.xlabel(f'ä¸»æˆåˆ†1 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.2%})')
        plt.ylabel(f'ä¸»æˆåˆ†2 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.2%})')
        plt.title('éŸ³ä¹é£æ ¼èšç±»å¯è§†åŒ–')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig('clustering_visualization.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return clusters, kmeans, scaler
    
    def feature_importance_analysis(self):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å˜å¼‚ç³»æ•° (æ ‡å‡†å·®/å‡å€¼)
        feature_importance = {}
        for col in self.feature_columns:
            if self.df[col].mean() != 0:
                cv = self.df[col].std() / abs(self.df[col].mean())
                feature_importance[col] = cv
        
        # æ’åºå¹¶æ˜¾ç¤º
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        print("ç‰¹å¾å˜å¼‚æ€§æ’åº (å˜å¼‚æ€§è¶Šå¤§ï¼ŒåŒºåˆ†åº¦è¶Šé«˜):")
        for feature, importance in sorted_features[:10]:
            print(f"{feature}: {importance:.3f}")
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        features, importances = zip(*sorted_features[:15])
        plt.figure(figsize=(12, 8))
        plt.barh(range(len(features)), importances)
        plt.yticks(range(len(features)), features)
        plt.xlabel('å˜å¼‚ç³»æ•°')
        plt.title('éŸ³ä¹ç‰¹å¾é‡è¦æ€§åˆ†æ')
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return sorted_features
    
    def generate_creation_suggestions(self):
        """åŸºäºåˆ†æç»“æœç”Ÿæˆåˆ›ä½œå»ºè®®"""
        print("\n=== ğŸµ éŸ³ä¹åˆ›ä½œå»ºè®® ===")
        
        # åˆ†æä½ å–œæ¬¢çš„éŸ³ä¹çš„å…±åŒç‰¹å¾
        bpm_mean = self.df['bpm'].mean()
        bpm_std = self.df['bpm'].std()
        
        onset_mean = self.df['onset_mean'].mean()
        onset_std = self.df['onset_std'].mean()
        
        mel_mean = self.df['mel_mean'].mean()
        
        print(f"\nğŸ¼ æ ¹æ®ä½ å–œæ¬¢çš„ {len(self.df)} é¦–æ­Œåˆ†æ:")
        print(f"â€¢ æ¨èBPMèŒƒå›´: {bpm_mean-bpm_std:.0f} - {bpm_mean+bpm_std:.0f}")
        print(f"â€¢ æœ€ä½³BPM: {bpm_mean:.0f}")
        
        if bpm_mean < 90:
            print("â€¢ é£æ ¼å€¾å‘: æ…¢æ­Œ/æŠ’æƒ… (é€‚åˆæ·±åº¦æƒ…æ„Ÿè¡¨è¾¾)")
        elif bpm_mean < 120:
            print("â€¢ é£æ ¼å€¾å‘: ä¸­é€Ÿ/æµè¡Œ (é€‚åˆæ—¥å¸¸è†å¬)")
        else:
            print("â€¢ é£æ ¼å€¾å‘: å¿«æ­Œ/èˆæ›² (é€‚åˆè¿åŠ¨å’Œæ´¾å¯¹)")
        
        print(f"\nğŸ¹ éŸ³è‰²å»ºè®®:")
        if onset_mean > 0.5:
            print("â€¢ èŠ‚å¥æ„Ÿå¼ºçƒˆï¼Œé€‚åˆåŠ å…¥æ‰“å‡»ä¹å™¨")
        else:
            print("â€¢ èŠ‚å¥æŸ”å’Œï¼Œé€‚åˆå¼¦ä¹å’Œé’¢ç´ä¸»å¯¼")
        
        if mel_mean > 0.01:
            print("â€¢ é¢‘è°±ä¸°å¯Œï¼Œå¯ä»¥ä½¿ç”¨å¤æ‚å’Œå£°")
        else:
            print("â€¢ é¢‘è°±ç®€æ´ï¼Œé€‚åˆç®€å•æ—‹å¾‹çº¿")
        
        # MFCCç‰¹å¾åˆ†æ
        mfcc_features = [col for col in self.feature_columns if 'mfcc' in col and 'mean' in col]
        mfcc_profile = [self.df[col].mean() for col in mfcc_features]
        
        print(f"\nğŸ¨ åˆ›ä½œå‚æ•°æ¨¡æ¿:")
        print(f"â€¢ BPM: {bpm_mean:.0f}")
        print(f"â€¢ èŠ‚å¥å¼ºåº¦: {onset_mean:.3f}")
        print(f"â€¢ æ¢…å°”é¢‘è°±å¼ºåº¦: {mel_mean:.6f}")
        print("â€¢ MFCCç‰¹å¾å‘é‡:", [f"{x:.3f}" for x in mfcc_profile[:5]])
    
    def save_analysis_report(self):
        """ä¿å­˜å®Œæ•´åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# éŸ³ä¹åˆ†ææŠ¥å‘Š\n")
        report.append(f"åˆ†ææ­Œæ›²æ•°é‡: {len(self.df)}\n")
        report.append(f"åˆ†ææ—¶é—´: {pd.Timestamp.now()}\n\n")
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        report.append("## åŸºç¡€ç»Ÿè®¡\n")
        report.append(f"å¹³å‡BPM: {self.df['bpm'].mean():.1f}\n")
        report.append(f"BPMæ ‡å‡†å·®: {self.df['bpm'].std():.1f}\n")
        report.append(f"å¹³å‡èŠ‚å¥å¼ºåº¦: {self.df['onset_mean'].mean():.3f}\n\n")
        
        # ä¿å­˜æŠ¥å‘Š
        with open('music_analysis_report.md', 'w', encoding='utf-8') as f:
            f.writelines(report)
        
        print("å®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° music_analysis_report.md")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = MusicPatternAnalyzer()
        
        # æ‰§è¡Œå„ç§åˆ†æ
        analyzer.basic_statistics()
        analyzer.correlation_analysis()
        analyzer.clustering_analysis(n_clusters=3)
        analyzer.feature_importance_analysis()
        analyzer.generate_creation_suggestions()
        analyzer.save_analysis_report()
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
        print("â€¢ correlation_matrix.png - ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾")
        print("â€¢ clustering_visualization.png - é£æ ¼èšç±»å›¾")
        print("â€¢ feature_importance.png - ç‰¹å¾é‡è¦æ€§å›¾")
        print("â€¢ music_analysis_report.md - å®Œæ•´åˆ†ææŠ¥å‘Š")
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ° analysis_results.csv æ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ analyze.py ç”ŸæˆéŸ³ä¹åˆ†ææ•°æ®")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main() 